{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72d8bb5",
   "metadata": {},
   "source": [
    "# Automating Job Search on LinkedIn with Python and Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034170d",
   "metadata": {},
   "source": [
    "In today's fast-paced world, job hunting can be a daunting task. Thankfully, automation can ease the burden by allowing you to search for jobs efficiently and effectively, all within the comfort of your Python environment.\n",
    "\n",
    "In this guide, we'll walk through a step-by-step process to automate LinkedIn job searches. By leveraging the power of Selenium, a popular web automation tool, and Python, a versatile programming language, you'll be able to search for job opportunities tailored to your preferences, industry, and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463195b",
   "metadata": {},
   "source": [
    "![](https://scrapfly.io/blog/content/images/web-scraping-with-selenium-and-python_banner_light.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623aaeb9",
   "metadata": {},
   "source": [
    "## Project Workflow:\n",
    "\n",
    "**1. Setting Up Your Environment:**\n",
    "\n",
    "- <u>Package Installation</u>: Install necessary packages such as Selenium and pandas using pip.\n",
    "\n",
    "- <u>Web Driver Configuration</u>: Configure the appropriate web driver (in this case, Chrome) for Selenium.\n",
    "\n",
    "**2. User Authentication:**\n",
    "\n",
    "- <u>LinkedIn Login</u>: Automate the login process using your LinkedIn username and password securely.\n",
    "\n",
    "- <u>Encrypted Input</u>: Use the getpass library to securely input your LinkedIn password without displaying it on the screen.\n",
    "\n",
    "**3. Job Search and Navigation:**\n",
    "\n",
    "- <u>Navigating to Jobs</u>: Access the LinkedIn jobs page and input your desired job domain or role.\n",
    "\n",
    "- <u>Pagination Handling</u>: Navigate through multiple pages of job listings, automatically scrolling and collecting data from each page.\n",
    "\n",
    "**4. Data Extraction and Storage:**\n",
    "\n",
    "- <u>Extracting Job Data</u>: Collect job titles, locations, companies, and corresponding URLs from the job listings.\n",
    "\n",
    "- <u>Data Structuring</u>: Organize the extracted data into lists or dictionaries for easy manipulation and storage.\n",
    "\n",
    "- <u>Error Handling</u>: Implement error handling to manage situations where specific elements might not be present on the page.\n",
    "\n",
    "**5. Exporting Data:**\n",
    "\n",
    "- <u>Creating a DataFrame</u>: Utilize the Pandas library to create a DataFrame from the extracted data.\n",
    "\n",
    "- <u>CSV Export</u>: Export the DataFrame to a CSV file, making it accessible for further analysis in tools like Excel or data science libraries in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127edcef",
   "metadata": {},
   "source": [
    "---\n",
    "## <center>Let's Begin!!</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5a6da",
   "metadata": {},
   "source": [
    "- Install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8150c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb017a87",
   "metadata": {},
   "source": [
    "- Import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a60c9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.actions.wheel_input import ScrollOrigin\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fd29d",
   "metadata": {},
   "source": [
    "- Initialize Selenium Webdriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "394c2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_driver():\n",
    "    \n",
    "#     options = Options()\n",
    "#     options.add_argument(\"--headless=new\")\n",
    "#     driver = webdriver.Chrome(options=options)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473fe7a",
   "metadata": {},
   "source": [
    "- Navigate to LinkedIn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41ae3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_linkedin(driver):\n",
    "    \n",
    "    driver.get(\"https://www.linkedin.com/feed/\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b587dd",
   "metadata": {},
   "source": [
    "- Sign in to LinkedIn.\n",
    "    - The **sign_in()** function requires the user to enter their LinkedIn username and password and clicks the sign-in button to authenticate the user and proceed to the next step.\n",
    "    \n",
    "    - ***Note:** If you run the code several times, you might be required to perform a CAPTCHA test manually to proceed ahead. Selenium will take care of rest of the process.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e4810a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_in(driver):\n",
    "    \n",
    "    sign_in = driver.find_element(By.XPATH, '/html/body/div[1]/main/div/p/a')\n",
    "    sign_in.click()\n",
    "    \n",
    "    uname = driver.find_element(By.ID, \"username\")\n",
    "    username = getpass.getpass(\"Enter your LinkedIn username: \")\n",
    "    uname.send_keys(username)\n",
    "    \n",
    "    pword = driver.find_element(By.ID, \"password\")\n",
    "    p = getpass.getpass(\"Enter your LinkedIn password: \")\n",
    "    pword.send_keys(p)\n",
    "    \n",
    "    final_sign_in = driver.find_element(By.XPATH, '//*[@id=\"organic-div\"]/form/div[3]/button')\n",
    "    final_sign_in.click()\n",
    "    \n",
    "    driver.implicitly_wait(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2f725",
   "metadata": {},
   "source": [
    "- Perform Job Search.\n",
    "    - The **perform_job_search()** function lets users enter the job they're looking for and takes them to the 'Jobs' section.\n",
    "    - First, we locate the 'Search Box' using its XPATH. Next, we take user input for the job search and press enter.\n",
    "    - Once the page loads, we find the 'Jobs' button using its XPATH and click on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "513f3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_job_search(driver):\n",
    "    \n",
    "    search_box = driver.find_element(By.XPATH, '//*[@id=\"global-nav-typeahead\"]/input')\n",
    "    job_search_domain = input(\"\\nPlease specify the job domain/job role that you are searching for: \\n\")\n",
    "    search_box.send_keys(job_search_domain)\n",
    "    search_box.send_keys(Keys.ENTER)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    jobs_button = driver.find_element(By.XPATH, '//button[text()=\"Jobs\"]')\n",
    "    jobs_button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    return job_search_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148411c",
   "metadata": {},
   "source": [
    "- Scrape Job Listings.\n",
    "    - The **scrape_jobs()** function checks if there are any jobs that match the user's criteria. If there are no matching jobs, it tells the user there are none.\n",
    "    - If jobs are available, it scrolls till the end of the page to load the HTML content of that page, extracts the required information, and loads the next page if present.\n",
    "    - We search for all the required tags using their XPATH, CLASS NAME, LINK TEXT, or CSS SELECTOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c0a7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs(driver):\n",
    "\n",
    "    # Now, we are on the jobs page\n",
    "    # Each page contains 25 jobs\n",
    "    # Thus, we want to visit page number 1,2,3,4\n",
    "    # to get the top 100 jobs\n",
    "    \n",
    "\n",
    "    # If we find 'No matching jobs found.' after job search, we print the same\n",
    "    # If we do not find 'No matching jobs found.', selenium will throw\n",
    "    # 'NoSuchElementException' Exception\n",
    "    # Thus, we use exception handling for scraping relevant information\n",
    "    try:\n",
    "        no_matching_jobs = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div[4]/div/div[1]/div/h1')\n",
    "        print('\\nNo matching jobs found.')\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        job_titles_list = []\n",
    "        job_links_list = []\n",
    "        job_locations_list = []\n",
    "        company_names_list = []\n",
    "        page_number = 2\n",
    "\n",
    "        print('\\n---> Job search begins....\\n')\n",
    "\n",
    "        for i in range(1, 5): # \n",
    "\n",
    "            print('Please wait, searching for jobs....')\n",
    "            \n",
    "            # The tag stored in footer variable helps us\n",
    "            # scroll at the bottom of the page\n",
    "            footer = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div/div[1]/div')\n",
    "            scroll_origin = ScrollOrigin.from_element(footer)\n",
    "            for i in range(6):\n",
    "                ActionChains(driver).scroll_from_origin(scroll_origin, 0, 550).perform()\n",
    "                time.sleep(1)\n",
    "\n",
    "            job_titles = driver.find_elements(By.CLASS_NAME, 'full-width.artdeco-entity-lockup__title.ember-view')\n",
    "            # job_titles_list = []\n",
    "            for job_title in job_titles:\n",
    "                job_titles_list.append(job_title.text)\n",
    "\n",
    "            # We extract the URL of a job posting\n",
    "            # By iterating over all the tags in 'job_titles[]' list\n",
    "            # And extracting the link associated with each job title\n",
    "\n",
    "            # job_links_list = []\n",
    "            for job_title in job_titles:\n",
    "                job_links_list.append(job_title.find_element(By.LINK_TEXT, job_title.text).get_attribute(\"href\"))        \n",
    "\n",
    "            job_locations = driver.find_elements(By.CLASS_NAME, 'artdeco-entity-lockup__caption')\n",
    "            # job_locations_list = []\n",
    "            for job_location in job_locations:\n",
    "                job_locations_list.append(job_location.text)        \n",
    "\n",
    "            company_names = driver.find_elements(By.CLASS_NAME, 'job-card-container__primary-description ')\n",
    "            # company_names_list = []\n",
    "            for company_name in company_names:\n",
    "                company_names_list.append(company_name.text)\n",
    "\n",
    "            try:\n",
    "                page_button = driver.find_element(By.CSS_SELECTOR, f'li[data-test-pagination-page-btn=\"{page_number}\"]').find_element(By.TAG_NAME, 'button')\n",
    "                page_button.click()\n",
    "                page_number += 1\n",
    "                time.sleep(2)\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "        \n",
    "        return job_titles_list, job_locations_list, company_names_list, job_links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c18d1",
   "metadata": {},
   "source": [
    "- Export to CSV.\n",
    "    - The **export_to_csv()** function creates a folder named 'Jobs', a Pandas DataFrame containing information about the top 100 jobs, and saves the DataFrame as a CSV file inside the 'Jobs' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "292eee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(job_search_domain, job_titles_list, job_locations_list, company_names_list, job_links_list):\n",
    "    \n",
    "    list_dict = {'Job Title': job_titles_list, 'Location': job_locations_list, 'Company': company_names_list, 'Link': job_links_list} \n",
    "    \n",
    "    folder_path = 'Jobs/'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df = pd.DataFrame(list_dict)\n",
    "    df.to_csv(os.path.join(folder_path, f\"{job_search_domain} Jobs.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a914f98",
   "metadata": {},
   "source": [
    "- Close the Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29e0d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_browser(driver):\n",
    "    print(f'\\nEnd Of Job Search!')\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef64ea1",
   "metadata": {},
   "source": [
    "## Let's start scraping the top 100 jobs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dd80817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your LinkedIn username: ········\n",
      "Enter your LinkedIn password: ········\n",
      "\n",
      "Please specify the job domain/job role that you are searching for: \n",
      "Machine Learning Engineer\n",
      "\n",
      "---> Job search begins....\n",
      "\n",
      "Please wait, searching for jobs....\n",
      "Please wait, searching for jobs....\n",
      "Please wait, searching for jobs....\n",
      "Please wait, searching for jobs....\n",
      "\n",
      "End Of Job Search!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    driver = init_driver()\n",
    "    navigate_to_linkedin(driver)\n",
    "    sign_in(driver)\n",
    "    job_search_domain = perform_job_search(driver)\n",
    "    information = scrape_jobs(driver)\n",
    "    export_to_csv(job_search_domain, *information)\n",
    "    close_browser(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fcb93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
